{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1da2da",
   "metadata": {},
   "source": [
    "* https://docs.pola.rs/user-guide/getting-started/#filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b9aaba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] shape=(2227480, 48), columns=['번호', '개방서비스명', '개방서비스아이디', '개방자치단체코드', '관리번호', '인허가일자', '인허가취소일자', '영업상태구분코드'] ...(+40 more)\n"
     ]
    },
    {
     "ename": "DTypePromotionError",
     "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDTypePromotionError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 241\u001b[0m\n\u001b[0;32m    228\u001b[0m categorical_features \u001b[38;5;241m=\u001b[39m [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m cat_cols \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m    229\u001b[0m lgbm \u001b[38;5;241m=\u001b[39m LGBMClassifier(\n\u001b[0;32m    230\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1500\u001b[39m,\n\u001b[0;32m    231\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.04\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    238\u001b[0m     n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    239\u001b[0m )\n\u001b[1;32m--> 241\u001b[0m \u001b[43mlgbm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m80\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_features\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pandas category 처리\u001b[39;49;00m\n\u001b[0;32m    247\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# ======================\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# 8) 평가 (ROC-AUC, PR-AUC, 최적 threshold)\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# ======================\u001b[39;00m\n\u001b[0;32m    252\u001b[0m proba \u001b[38;5;241m=\u001b[39m lgbm\u001b[38;5;241m.\u001b[39mpredict_proba(X_test)[:, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mf:\\project_test\\mlops\\.venv\\lib\\site-packages\\lightgbm\\sklearn.py:1560\u001b[0m, in \u001b[0;36mLGBMClassifier.fit\u001b[1;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1557\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1558\u001b[0m             valid_sets\u001b[38;5;241m.\u001b[39mappend((valid_x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_le\u001b[38;5;241m.\u001b[39mtransform(valid_y)))\n\u001b[1;32m-> 1560\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1562\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1565\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1566\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1567\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_class_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_class_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1573\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1574\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1575\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1576\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mf:\\project_test\\mlops\\.venv\\lib\\site-packages\\lightgbm\\sklearn.py:1049\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[1;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[0;32m   1046\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1047\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[1;32m-> 1049\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1055\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1056\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1057\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1058\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# This populates the property self.n_features_, the number of features in the fitted model,\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# and so should only be set after fitting.\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;66;03m# The related property self._n_features_in, which populates self.n_features_in_,\u001b[39;00m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;66;03m# is set BEFORE fitting.\u001b[39;00m\n\u001b[0;32m   1065\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mnum_feature()\n",
      "File \u001b[1;32mf:\\project_test\\mlops\\.venv\\lib\\site-packages\\lightgbm\\engine.py:297\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# construct booster\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 297\u001b[0m     booster \u001b[38;5;241m=\u001b[39m \u001b[43mBooster\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    298\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    299\u001b[0m         booster\u001b[38;5;241m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mf:\\project_test\\mlops\\.venv\\lib\\site-packages\\lightgbm\\basic.py:3656\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str)\u001b[0m\n\u001b[0;32m   3649\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_network(\n\u001b[0;32m   3650\u001b[0m         machines\u001b[38;5;241m=\u001b[39mmachines,\n\u001b[0;32m   3651\u001b[0m         local_listen_port\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocal_listen_port\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3652\u001b[0m         listen_time_out\u001b[38;5;241m=\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_out\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m120\u001b[39m),\n\u001b[0;32m   3653\u001b[0m         num_machines\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_machines\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   3654\u001b[0m     )\n\u001b[0;32m   3655\u001b[0m \u001b[38;5;66;03m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 3656\u001b[0m \u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3657\u001b[0m \u001b[38;5;66;03m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m params\u001b[38;5;241m.\u001b[39mupdate(train_set\u001b[38;5;241m.\u001b[39mget_params())\n",
      "File \u001b[1;32mf:\\project_test\\mlops\\.venv\\lib\\site-packages\\lightgbm\\basic.py:2590\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2585\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_init_score_by_predictor(\n\u001b[0;32m   2586\u001b[0m                 predictor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predictor, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, used_indices\u001b[38;5;241m=\u001b[39mused_indices\n\u001b[0;32m   2587\u001b[0m             )\n\u001b[0;32m   2588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2589\u001b[0m     \u001b[38;5;66;03m# create train\u001b[39;00m\n\u001b[1;32m-> 2590\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   2594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2596\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2597\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predictor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2598\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2601\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2602\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfree_raw_data:\n\u001b[0;32m   2604\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\project_test\\mlops\\.venv\\lib\\site-packages\\lightgbm\\basic.py:2123\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, feature_name, categorical_feature, params, position)\u001b[0m\n\u001b[0;32m   2121\u001b[0m     categorical_feature \u001b[38;5;241m=\u001b[39m reference\u001b[38;5;241m.\u001b[39mcategorical_feature\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd_DataFrame):\n\u001b[1;32m-> 2123\u001b[0m     data, feature_name, categorical_feature, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpandas_categorical \u001b[38;5;241m=\u001b[39m \u001b[43m_data_from_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2125\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2126\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2127\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpandas_categorical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpandas_categorical\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2128\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2129\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m _is_pyarrow_table(data) \u001b[38;5;129;01mand\u001b[39;00m feature_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2130\u001b[0m     feature_name \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcolumn_names\n",
      "File \u001b[1;32mf:\\project_test\\mlops\\.venv\\lib\\site-packages\\lightgbm\\basic.py:865\u001b[0m, in \u001b[0;36m_data_from_pandas\u001b[1;34m(data, feature_name, categorical_feature, pandas_categorical)\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;66;03m# so that the target dtype considers floats\u001b[39;00m\n\u001b[0;32m    864\u001b[0m df_dtypes\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m--> 865\u001b[0m target_dtype \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdf_dtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    868\u001b[0m     _pandas_to_numpy(data, target_dtype\u001b[38;5;241m=\u001b[39mtarget_dtype),\n\u001b[0;32m    869\u001b[0m     feature_name,\n\u001b[0;32m    870\u001b[0m     categorical_feature,\n\u001b[0;32m    871\u001b[0m     pandas_categorical,\n\u001b[0;32m    872\u001b[0m )\n",
      "\u001b[1;31mDTypePromotionError\u001b[0m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Int16DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Int8DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Float32DType'>)"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# pandas + LightGBM 폐업예측 파이프라인 (견고한 버전)\n",
    "import os, json, pickle, warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    classification_report, roc_auc_score, average_precision_score,\n",
    "    precision_recall_curve, f1_score, confusion_matrix\n",
    ")\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb  # callbacks용\n",
    "\n",
    "# ======================\n",
    "# 설정\n",
    "# ======================\n",
    "CSV_PATH = \"./data/restaurant_20_25.csv\"  # 경로 확인\n",
    "TARGET_CANDIDATES = [\"영업상태명\"]\n",
    "ID_LIKE_PATTERNS = [\"사업자\", \"business\", \"등록번호\", \"id\", \"아이디\", \"uuid\"]\n",
    "RARE_MIN_COUNT = 20\n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "ARTIFACT_DIR = \"./data/lgbm_closure\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "\n",
    "# ======================\n",
    "# 유틸\n",
    "# ======================\n",
    "def read_csv_safely(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    1) UTF-8 시도 → 실패시 cp949로 재시도\n",
    "    2) 크고 지저분한 파일 대비 on_bad_lines='skip'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(path, encoding=\"utf-8\", on_bad_lines=\"skip\", low_memory=False)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"cp949\", on_bad_lines=\"skip\", low_memory=False)\n",
    "\n",
    "def find_target(colnames):\n",
    "    low = [c.lower() for c in colnames]\n",
    "    for t in TARGET_CANDIDATES:\n",
    "        if t.lower() in low:\n",
    "            return colnames[low.index(t.lower())]\n",
    "    raise ValueError(f\"타깃 컬럼을 찾지 못했습니다. 후보: {TARGET_CANDIDATES}\")\n",
    "\n",
    "def looks_like_id(name: str) -> bool:\n",
    "    n = name.lower()\n",
    "    if n in (\"index\",):\n",
    "        return True\n",
    "    for pat in ID_LIKE_PATTERNS:\n",
    "        if pat.lower() in n:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def compress_rare_categories(s: pd.Series, min_count: int) -> pd.Series:\n",
    "    vc = s.value_counts(dropna=False)\n",
    "    rare_values = set(vc[vc < min_count].index)\n",
    "    if not rare_values:\n",
    "        return s\n",
    "    return s.where(~s.isin(rare_values), \"__RARE__\")\n",
    "\n",
    "def auto_datetime_parse(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    이름 패턴 기반으로 날짜 후보 컬럼들을 errors='coerce'로 유연 파싱(NaT 처리)\n",
    "    \"\"\"\n",
    "    date_like_keywords = (\"일자\", \"date\", \"날짜\", \"개업\", \"등록\", \"인허가\")\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if any(k in cl for k in date_like_keywords):\n",
    "            # 'YYYY-MM-DD...' 앞 10자리만 남기기\n",
    "            df[c] = df[c].astype(str).str.slice(0, 10)\n",
    "            df[c] = pd.to_datetime(df[c], errors=\"coerce\", format=\"%Y-%m-%d\")\n",
    "    return df\n",
    "\n",
    "def add_derived_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    now_year = datetime.now().year\n",
    "    # 개업연도 → 영업연수\n",
    "    if \"개업연도\" in df.columns:\n",
    "        # 숫자형만 처리\n",
    "        if pd.api.types.is_numeric_dtype(df[\"개업연도\"]):\n",
    "            df[\"영업연수\"] = (now_year - df[\"개업연도\"].astype(float)).astype(float)\n",
    "    # 개업일자/인허가일자 → 영업연수\n",
    "    for date_col in [\"개업일자\", \"인허가일자\"]:\n",
    "        if date_col in df.columns and pd.api.types.is_datetime64_any_dtype(df[date_col]):\n",
    "            df[\"영업연수\"] = (now_year - df[date_col].dt.year).astype(float)\n",
    "    return df\n",
    "\n",
    "def auto_log1p(df: pd.DataFrame, num_cols, skew_thr: float = 1.0):\n",
    "    \"\"\"\n",
    "    양의 왜도 큰(> skew_thr) & 최소값>=0 인 수치에 log1p 파생\n",
    "    \"\"\"\n",
    "    to_log = []\n",
    "    for c in num_cols:\n",
    "        col = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if col.notna().sum() == 0:\n",
    "            continue\n",
    "        mn = col.min()\n",
    "        sk = col.skew()\n",
    "        if pd.notna(sk) and sk > skew_thr and pd.notna(mn) and mn >= 0:\n",
    "            to_log.append(c)\n",
    "    for c in to_log:\n",
    "        df[f\"{c}_log1p\"] = np.log1p(pd.to_numeric(df[c], errors=\"coerce\")).astype(float)\n",
    "    return df\n",
    "\n",
    "def infer_time_split_key(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    날짜/연도 관련 컬럼 중 하나를 분할 기준으로 선택\n",
    "    \"\"\"\n",
    "    for c in df.columns:\n",
    "        cl = c.lower()\n",
    "        if pd.api.types.is_datetime64_any_dtype(df[c]) or any(k in cl for k in [\"년\", \"연도\", \"date\", \"날짜\", \"개업\", \"open\", \"인허가\"]):\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "# ======================\n",
    "# 1) 로드 & 타깃 식별\n",
    "# ======================\n",
    "df = read_csv_safely(CSV_PATH)\n",
    "print(f\"[INFO] shape={df.shape}, columns={list(df.columns)[:8]} ...(+{len(df.columns)-8} more)\" if len(df.columns)>8 else f\"[INFO] shape={df.shape}, columns={list(df.columns)}\")\n",
    "\n",
    "TARGET = find_target(df.columns)\n",
    "df = df[~df[TARGET].isna()].copy()\n",
    "\n",
    "# 타깃을 0/1로 보정 (bool/문자형 모두 커버)\n",
    "if df[TARGET].dtype == \"bool\":\n",
    "    df[TARGET] = df[TARGET].astype(int)\n",
    "elif pd.api.types.is_numeric_dtype(df[TARGET]):\n",
    "    df[TARGET] = (df[TARGET].astype(float) > 0).astype(int)  # 0/1/… → 0/1\n",
    "else:\n",
    "    # 문자열 매핑\n",
    "    pos_values = {\"1\", \"true\", \"t\", \"y\", \"yes\", \"폐업\", \"closed\"}\n",
    "    df[TARGET] = df[TARGET].astype(str).str.lower().isin(pos_values).astype(int)\n",
    "\n",
    "# ======================\n",
    "# 2) 날짜 파싱 & 파생\n",
    "# ======================\n",
    "df = auto_datetime_parse(df)\n",
    "df = add_derived_features(df)\n",
    "\n",
    "# ======================\n",
    "# 3) 컬럼 타입 분류/정리\n",
    "# ======================\n",
    "# ID/고유키 제거 (정보누설 방지)\n",
    "drop_cols = [c for c in df.columns if looks_like_id(c) and c != TARGET]\n",
    "if drop_cols:\n",
    "    df.drop(columns=drop_cols, inplace=True, errors=\"ignore\")\n",
    "\n",
    "# 범주/수치 후보\n",
    "cat_cols = [c for c in df.columns if c != TARGET and (df[c].dtype == \"object\" or pd.api.types.is_string_dtype(df[c]))]\n",
    "num_cols = [c for c in df.columns if c != TARGET and pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "# 결측치 처리\n",
    "# - 수치: 중앙값\n",
    "for c in num_cols:\n",
    "    med = pd.to_numeric(df[c], errors=\"coerce\").median()\n",
    "    if pd.isna(med):\n",
    "        med = 0.0\n",
    "    df[c] = pd.to_numeric(df[c], errors=\"coerce\").fillna(med)\n",
    "\n",
    "# - 범주: \"__MISSING__\" + 희귀 카테고리 묶기\n",
    "for c in cat_cols:\n",
    "    df[c] = df[c].astype(\"string\").fillna(\"__MISSING__\")\n",
    "    df[c] = compress_rare_categories(df[c], RARE_MIN_COUNT)\n",
    "\n",
    "# 왜도 큰 수치 log1p 파생\n",
    "df = auto_log1p(df, num_cols, skew_thr=1.0)\n",
    "\n",
    "# pandas category 부여 (LightGBM 네이티브 카테고리 처리)\n",
    "for c in cat_cols:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].astype(\"category\")\n",
    "\n",
    "# ======================\n",
    "# 4) 분할 방식 결정(시간 분할 우선)\n",
    "# ======================\n",
    "split_key = infer_time_split_key(df)\n",
    "use_time_split = False\n",
    "if split_key is not None:\n",
    "    # 연도형 문자열/숫자라면 숫자로 변환 시도\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[split_key]):\n",
    "        # 연도로 보이면 숫자 변환\n",
    "        df[\"_split_key_num_\"] = pd.to_numeric(df[split_key], errors=\"coerce\")\n",
    "        key_series = df[\"_split_key_num_\"]\n",
    "    else:\n",
    "        key_series = pd.to_datetime(df[split_key], errors=\"coerce\").view(\"int64\")  # NaT→NaN 예방용 int64\n",
    "    # NaN 비율이 과하면 시간분할 포기\n",
    "    if key_series.notna().mean() >= 0.7:\n",
    "        use_time_split = True\n",
    "    else:\n",
    "        split_key = None\n",
    "        if \"_split_key_num_\" in df.columns:\n",
    "            df.drop(columns=[\"_split_key_num_\"], inplace=True)\n",
    "\n",
    "# ======================\n",
    "# 5) 특징/타깃 분리\n",
    "# ======================\n",
    "X_cols = [c for c in df.columns if c != TARGET]\n",
    "X = df[X_cols].copy()\n",
    "y = df[TARGET].astype(int).copy()\n",
    "\n",
    "# ======================\n",
    "# 6) 학습/검증 분할\n",
    "# ======================\n",
    "if use_time_split:\n",
    "    if pd.api.types.is_datetime64_any_dtype(df[split_key]):\n",
    "        order = df[split_key].argsort(kind=\"mergesort\")  # 안정 정렬\n",
    "    elif \"_split_key_num_\" in df.columns:\n",
    "        order = df[\"_split_key_num_\"].argsort(kind=\"mergesort\")\n",
    "    else:\n",
    "        order = X.index.to_series().argsort(kind=\"mergesort\")\n",
    "    cutoff = int(len(df) * (1.0 - TEST_SIZE))\n",
    "    idx_train = order.iloc[:cutoff].index\n",
    "    idx_test  = order.iloc[cutoff:].index\n",
    "    X_train, X_test = X.loc[idx_train], X.loc[idx_test]\n",
    "    y_train, y_test = y.loc[idx_train], y.loc[idx_test]\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "# ======================\n",
    "# 7) LightGBM 학습 (불균형 보정 + 조기종료)\n",
    "# ======================\n",
    "categorical_features = [c for c in cat_cols if c in X_train.columns]\n",
    "lgbm = LGBMClassifier(\n",
    "    n_estimators=1500,\n",
    "    learning_rate=0.04,\n",
    "    num_leaves=63,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    reg_lambda=1.0,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgbm.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric=\"auc\",\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=80, verbose=True)],\n",
    "    categorical_feature=categorical_features  # pandas category 처리\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 8) 평가 (ROC-AUC, PR-AUC, 최적 threshold)\n",
    "# ======================\n",
    "proba = lgbm.predict_proba(X_test)[:, 1]\n",
    "pred_default = (proba >= 0.5).astype(int)\n",
    "\n",
    "roc = roc_auc_score(y_test, proba)\n",
    "pr_auc = average_precision_score(y_test, proba)\n",
    "\n",
    "prec, rec, thr = precision_recall_curve(y_test, proba)\n",
    "f1s = 2 * (prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = int(np.nanargmax(f1s))\n",
    "best_thr = 0.5 if best_idx >= len(thr) else float(thr[best_idx])\n",
    "pred_best = (proba >= best_thr).astype(int)\n",
    "\n",
    "print(\"\\n[기본 threshold=0.5]\")\n",
    "print(classification_report(y_test, pred_default))\n",
    "print(\"ROC-AUC:\", roc)\n",
    "print(\"PR-AUC :\", pr_auc)\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred_default))\n",
    "\n",
    "print(\"\\n[최적 F1 threshold=%.4f]\" % best_thr)\n",
    "print(classification_report(y_test, pred_best))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, pred_best))\n",
    "\n",
    "# ======================\n",
    "# 9) 중요 변수 상위 20 (gain)\n",
    "# ======================\n",
    "booster = lgbm.booster_\n",
    "gain_importance = booster.feature_importance(importance_type=\"gain\")\n",
    "features = booster.feature_name()\n",
    "order = np.argsort(gain_importance)[::-1]\n",
    "topk = min(20, len(features))\n",
    "print(\"\\n[Feature Importance - gain 기준 TOP %d]\" % topk)\n",
    "for i in range(topk):\n",
    "    j = order[i]\n",
    "    print(f\"{i+1:2d}. {features[j]}  gain={gain_importance[j]:.2f}\")\n",
    "\n",
    "# ======================\n",
    "# 10) 아티팩트 저장 (모델/메타/임계값)\n",
    "# ======================\n",
    "with open(os.path.join(ARTIFACT_DIR, \"model_lgbm.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(lgbm, f)\n",
    "\n",
    "meta = {\n",
    "    \"target\": TARGET,\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"dropped_id_cols\": [c for c in drop_cols if c in X_cols],\n",
    "    \"time_split_used\": use_time_split,\n",
    "    \"time_col\": split_key,\n",
    "    \"best_threshold\": best_thr,\n",
    "    \"columns_final\": X_cols\n",
    "}\n",
    "with open(os.path.join(ARTIFACT_DIR, \"preprocess_meta.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n[저장 완료] 모델: {os.path.join(ARTIFACT_DIR,'model_lgbm.pkl')}\")\n",
    "print(f\"[저장 완료] 메타 : {os.path.join(ARTIFACT_DIR,'preprocess_meta.json')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52305fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
